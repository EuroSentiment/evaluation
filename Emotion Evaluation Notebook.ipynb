{
 "metadata": {
  "name": "",
  "signature": "sha256:d389f84ccbdba0935f824efd84dbc788959e15f07a60dfccc78f9cee991f5990"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Conversion"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "import logging\n",
      "logger = logging.getLogger()\n",
      "print = logger.debug\n",
      "import os\n",
      "import os.path\n",
      "import fnmatch\n",
      "import json\n",
      "\n",
      "import itertools\n",
      "\n",
      "from IPython.display import display\n",
      "\n",
      "ORIGINAL_FOLDER = \"emotion/original\"\n",
      "CORPORA_FOLDER = \"emotion/corpora\"\n",
      "FILE_RAW = \"Raw_services_emotion.csv\"\n",
      "FILE_PROCESSED = \"Processed_services_emotion.csv\"\n",
      "\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!pip install xlrd\n",
      "#!pip install unicodecsv\n",
      "!pip install -U scikit-learn"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Paradigma"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import listdir\n",
      "from os.path import isfile, join\n",
      "\n",
      "import xlrd\n",
      "import unicodecsv\n",
      "import codecs\n",
      "\n",
      "def convert_pt(infile, input_folder=\"/\", output_folder=os.path.join(CORPORA_FOLDER, \"PT\")):\n",
      "    new_file = os.path.join(output_folder,\"{}.{}\".format(infile.rsplit(\".\",1)[0], \"csv\"))\n",
      "    with codecs.open(new_file, 'wb') as csvfile:\n",
      "        cwriter = unicodecsv.writer(csvfile, quoting=unicodecsv.QUOTE_ALL)\n",
      "        f = xlrd.open_workbook(os.path.join(input_folder, infile), on_demand=True)\n",
      "        sheet=f.sheet_by_index(0) \n",
      "        for i in range(1, sheet.nrows):\n",
      "            row= sheet.row_values(i)\n",
      "            if len(row[0]) > 6:\n",
      "                try:\n",
      "                    emotion = row[3]\n",
      "                    cwriter.writerow([row[0].encode(\"utf-8\").strip(), emotion])\n",
      "                except Exception as ex:\n",
      "                        logger.error(\"Error: {} {}\".format(row, ex))\n",
      "\n",
      "mypath = os.path.join(ORIGINAL_FOLDER, \"PT\")\n",
      "pt_files = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
      "\n",
      "for i in pt_files:\n",
      "    print(i)\n",
      "    convert_pt(i, input_folder=mypath)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Expert System"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import listdir\n",
      "from os.path import isfile, join\n",
      "\n",
      "import xlrd\n",
      "import csv\n",
      "import codecs\n",
      "\n",
      "def convert_es(infile, input_folder=\"/\", output_folder=os.path.join(CORPORA_FOLDER, \"ES\")):\n",
      "    new_file = os.path.join(output_folder,\"{}.{}\".format(infile.rsplit(\".\",1)[0], \"csv\"))\n",
      "    with codecs.open(new_file, 'wb') as csvfile:\n",
      "        cwriter = unicodecsv.writer(csvfile, quoting=unicodecsv.QUOTE_ALL)\n",
      "        f = xlrd.open_workbook(os.path.join(input_folder, infile), on_demand=True)\n",
      "        sheet=f.sheet_by_index(0) \n",
      "        for i in range(1, sheet.nrows):\n",
      "            row= sheet.row_values(i)\n",
      "            if len(row[0]) > 6:\n",
      "                try:\n",
      "                    emotion = row[3]\n",
      "                    cwriter.writerow([row[0].encode(\"utf-8\").strip(), emotion])\n",
      "                except Exception as ex:\n",
      "                        logger.error(\"Error: {} {}\".format(row, ex))\n",
      "        \n",
      "mypath = os.path.join(ORIGINAL_FOLDER, \"ES\")\n",
      "es_files = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
      "\n",
      "for i in es_files:\n",
      "    print(i)\n",
      "    convert_es(i, input_folder=mypath)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Calling the services"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "logger.setLevel(logging.WARN)\n",
      "\n",
      "import requests\n",
      "import grequests\n",
      "import gevent\n",
      "import unicodecsv\n",
      "import time\n",
      "\n",
      "from collections import defaultdict\n",
      "from functools import partial\n",
      "# If using requests > v0.13.0, use\n",
      "# from grequests import async\n",
      "\n",
      "EUROSENTIMENT_TOKEN = \"78cab860-d17d-4afa-b1ac-d6ff84e942d7\"\n",
      "\n",
      "print(\"Using TOKEN: {}\".format(EUROSENTIMENT_TOKEN))\n",
      "\n",
      "SERVICES = {\n",
      "           #  \"test\": [\"http://eurosentiment-endpoint.herokuapp.com/\",],\n",
      "            \"PT\": [\"http://217.26.90.243:8080/EuroSentimentServices/services/server/access/sptse0208\",],\n",
      "            \"ES\": [\"http://217.26.90.243:8080/EuroSentimentServices/services/server/access/sesse0328\",]\n",
      "}\n",
      "\n",
      "\n",
      "results = pd.DataFrame(columns=(\"corpus\",))\n",
      "\n",
      "\n",
      "def add_result(response, verify=None, proxies=None, cert=None, stream=None, **kwargs):\n",
      "    if 300 >= response.status_code >= 200 or 500 <= response.status_code:\n",
      "        global results\n",
      "        is_json = True\n",
      "        try:\n",
      "            result = json.loads(response.content)\n",
      "            print(\"Ok: {}\".format(result))\n",
      "        except Exception:\n",
      "            print(\"Fail: {url} - {response}\".format(url=response.url,\n",
      "                                                response=response.content))\n",
      "            result = response.content\n",
      "            is_json = False\n",
      "        args = kwargs.copy()\n",
      "        args.update({\"result\": response.content,\n",
      "                    \"code\": response.status_code,\n",
      "                    \"is_json\": is_json,\n",
      "                    \"headers\": str(response.request.headers)})\n",
      "        newrow = pd.DataFrame(data=args, index=[1])\n",
      "        print(\"dict: {}\".format(args))\n",
      "        print(\"Adding row:\")\n",
      "        print(newrow)\n",
      "        try:\n",
      "            results = results.append(newrow , ignore_index=True)    \n",
      "        except Exception as ex:\n",
      "            logger.error(\"Error adding: {}\".format(ex))\n",
      "    else:\n",
      "        print(\"Status code: {}\".format(response.status_code))\n",
      "        print(\"Response: {}\".format(response.content))\n",
      "\n",
      "\n",
      "def process(service, session=None, **kwargs):\n",
      "    print(\"Getting emotion with {}\".format(kwargs))\n",
      "    headers = {'x-eurosentiment-token': EUROSENTIMENT_TOKEN}\n",
      "    args = kwargs.copy()\n",
      "    action_item = grequests.post(service,\n",
      "                                 data={\"input\": kwargs[\"input\"]},\n",
      "                                 headers=headers,\n",
      "                                 timeout=90,\n",
      "                                 session=session,\n",
      "                                 hooks = {\"response\": partial(add_result,\n",
      "                                                              service=service,\n",
      "                                                              **args) })\n",
      "    return action_item\n",
      "\n",
      "def process_corpus(corpus_file, max_entries=None, **kwargs):\n",
      "    session = requests.Session()\n",
      "    pool_size = 50\n",
      "    session.mount('http://', requests.adapters.HTTPAdapter(pool_connections=pool_size, pool_maxsize=pool_size))\n",
      "    args = kwargs.copy()\n",
      "    async_list = []\n",
      "    with open(corpus_file) as f:\n",
      "        creader = unicodecsv.reader(f)\n",
      "        for row in itertools.islice(creader, max_entries):\n",
      "            entry = row[0].replace(\"\\\"\", \"\")\n",
      "            emotion = row[1]\n",
      "            for provider in SERVICES:\n",
      "                for service in SERVICES[provider]:\n",
      "                    print(\"Adding for {} and {}\".format(provider, service))\n",
      "                    action_item = process(  session=session,\n",
      "                                            service=service,\n",
      "                                            input=entry,\n",
      "                                            annotatedEmotion=emotion,\n",
      "                                            provider=provider,\n",
      "                                            corpus=corpus_file,\n",
      "                                            **args)\n",
      "                    async_list.append(action_item)\n",
      "    for idx in xrange(0, len(async_list), pool_size):\n",
      "        grequests.map(async_list[idx:idx+pool_size], size=pool_size, exception_handler=error_handler)\n",
      "        time.sleep(5)\n",
      "    session.close()\n",
      "    corpus_statistics(corpus_file)\n",
      "\n",
      "def exception_handler(request, *args):\n",
      "    logger.error(dir(request))\n",
      "    \n",
      "def error_handler(*args, **kwargs):\n",
      "    logger.error(\"Error: {} {}\".format(args, kwargs))\n",
      "\n",
      "def corpus_statistics(corpus_file, with_results=True):\n",
      "    logger.warn(\"#\"*20)\n",
      "    logger.warn(\"Statistics for corpus {}\".format(corpus_file))\n",
      "    logger.warn(\"-\"*20)\n",
      "    with open(corpus_file) as f:\n",
      "        creader = unicodecsv.reader(f)\n",
      "        rows = list(creader)\n",
      "    if with_results:\n",
      "        logger.warn(\"Entries: {}\".format(len(rows)))\n",
      "        logger.warn(\"Entries read: {}\".format(len(results[results['corpus'] == corpus_file])))\n",
      "        logger.warn(\"Total: {}\".format(len(results)))\n",
      "    logger.warn(\"#\"*20)\n",
      "\n",
      "\n",
      "    \n",
      "def process_all_corpora(folder=CORPORA_FOLDER, max_entries=None):\n",
      "    for root, dirnames, filenames in os.walk(folder):\n",
      "      for filename in fnmatch.filter(filenames, '*.csv'):\n",
      "          logger.warn(\"Processing: {}\".format(filename))\n",
      "          process_corpus(os.path.join(root, filename), max_entries)\n",
      "\n",
      "process_all_corpora()\n",
      "display(results)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "results.to_csv(FILE_RAW, encoding='utf-8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Metrics for emotions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Testing emotions is tricker than sentiment, because there might be several emotions for each entry. This is just a naive method, but can be further extended"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "logger.setLevel(logging.DEBUG)\n",
      "from IPython.html.widgets import interactive\n",
      "\n",
      "def get_first(target):\n",
      "    if isinstance(target, list):\n",
      "        if len(target)>0:\n",
      "            return target[0]\n",
      "        else:\n",
      "            return None\n",
      "    else:\n",
      "        return target\n",
      "    \n",
      "def get_emotion(value):\n",
      "    try:\n",
      "        j = json.loads(value)\n",
      "        entry0 = get_first(j[\"entries\"])\n",
      "        emotion0 = get_first(entry0[\"emotions\"])\n",
      "        if emotion0 and \"onyx:hasEmotion\" in emotion0:\n",
      "            return  \";\".join(sorted(emo.get(\"onyx:hasEmotionCategory\", None) for emo in emotion0[\"onyx:hasEmotion\"]))\n",
      "        else:\n",
      "            return [\"n/c\".]\n",
      "    except Exception as ex:\n",
      "        #logger.error(\"EXCEPTION {}\".format(ex))\n",
      "        return \"n/c\".\n",
      "\n",
      "\n",
      "df = pd.DataFrame.from_csv(FILE_RAW)\n",
      "df[\"emotion\"] = ['result'].map(get_emotion)\n",
      "cols = df.columns.tolist()\n",
      "cols.append(cols.pop(0))\n",
      "df = df[cols]\n",
      "df.to_csv(FILE_PROCESSED)\n",
      "\n",
      "\n",
      "from sklearn.metrics import classification_report\n",
      "\n",
      "services = df[\"service\"].unique()\n",
      "for service in services:\n",
      "    print(\"#\"*20)\n",
      "    print(\"Results for service: {}\".format(service))\n",
      "    sdf = df[df[\"service\"] == service]\n",
      "    corpora = sdf[\"corpus\"].unique()\n",
      "    for corpus in corpora:\n",
      "        scdf = sdf[sdf[\"corpus\"] == corpus]\n",
      "        print(\"Corpus: {}\".format(corpus))\n",
      "        print(classification_report(scdf[\"annotatedEmotion\"], scdf[\"emotion\"]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Metrics for entities"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "df = pd.DataFrame.from_csv(FILE_RAW)\n",
      "df = df[df[\"is_json\"] == True]\n",
      "def get_entities(value):\n",
      "    try:\n",
      "        j = json.loads(value)\n",
      "        entities = set()\n",
      "        emotion_entities = set()\n",
      "        if \"entries\" in j:\n",
      "            for entry in j[\"entries\"]:\n",
      "                for string in entry[\"strings\"]:\n",
      "                    entities.add(string[\"itsrdf:taIdentRef\"])\n",
      "                for emotion in entry[\"emotions\"]:\n",
      "                    if \"onyx:aboutObject\" in opinion:\n",
      "                        emotion_entities.add(opinion[\"onyx:aboutObject\"])\n",
      "        return entities, emotion_entities\n",
      "    except Exception as ex:\n",
      "        #logger.error(\"ERROR {}\".format(ex))\n",
      "        return (), ()\n",
      "    \n",
      "def get_namespace(entities):\n",
      "    return [entity.split(\"#\")[0] for entity in entities ]\n",
      "    \n",
      "namespaces = set()\n",
      "df[\"entities\"], df[\"emotion_entities\"] = zip(*df[\"result\"].map(get_entities))\n",
      "for i in df[\"entities\"]:\n",
      "    namespaces.update([entity.split(\"#\")[0] for entity in i])\n",
      "display(namespaces)\n",
      "\n",
      "emonamespaces = set()\n",
      "for i in df[\"emotion_entities\"]:\n",
      "    emonamespaces.update([entity for entity in i])\n",
      "display(emonamespaces)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}